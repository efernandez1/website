<!DOCTYPE html>
<html lang="en">
<head>
  <link href="style.css" rel="stylesheet"/>
</head>
<body style = "background-color: rgb(204, 204, 250)">
<body>
<body>
  <div class="top">
    <h1>Alpha Analytics</h1>
    <img src="blue.png" width="600" height="300">
  </div>
  <div class="sideBySide">
    <div id="LC">
      <div>
        <h2><a href="index.html">Home</a></h2>
      </div>
      <div>
        <h2><a href="analysis.html">Analysis</a></h2>
      </div>
      <div>
        <h2><a href="blog.html">Blog</a></h2>
      </div>
      <div>
        <h2><a href="resources.html">Resources</a></h2>
      </div>
    </div>
    <div id="RC">
      <p>Titanic analysis
<p>
This is a classic machine learning analysis on Kaggle.
In fact, its one of the first machine learning data sets I came across.
<p>
The data set can be found here and as with all Kaggle competitions,
the data is already partitioned into a training a test dataset.
For more information on why this is important, see my blog.
(write blog about cross validation techniques)
<p>
With every analysis it’s crucial to understand the data you are dealing with.
Sometimes this is not possible because the information is not there and so you
will probably deploy some unsupervised learning techniques on the data.
Anyways, the titanic data is 11 columns wide by XXX rows long.
More can be found on the individual covariates here (link).
<p>
The goal of this analysis is to come up with a model to accurately predict
whether or not a passenger would have survived the tragic sinking of the
titanic’s maiden voyage in 1912 where 1502 of 2224 passengers and crew died.
<p>
Lets use some basic summary statistics and visuals to try and better understand the data.
<p>
Without doing any feature engineering, lets see how well a GBM or RF model will perform.
</p>
<h2><a href="testing.html">Titanic</a></h2>
<p>
San Francisco Crime
<p>
The next analysis I’m going to take a look at is another Kaggle generated data set.
The goal of this analysis is to train a model to predict the category of a crime
(theft, assault, robbery, etc) based on time, day of week, and location in San Francisco.
The training data set is 9 columns wide and 900,000 rows long. Have this rich of
a data set allows for a solid analysis. Additionally, based on the dataset,
it looks like we will be able to run some cool algorithms that will help build our model.
<p>
As always, lets get some summary statistics and look at some data visualizations.

<p>
Now its time to train a model…
<p>
Lets see how our model does against out of sample data.
<p>
Overall, not too bad for a quick analysis using some pretty advanced techniques… </p>

      <p>This will be a collection of analysis on important issues. Most recently gun control. What does the data say about gun control laws? Is it a good idea? I will run through various analysis transforming raw data into clean data sets that I explore and use machine learning techniques to gleam new insights. </p>

      <p>This will be a collection of analysis on important issues. Most recently gun control. What does the data say about gun control laws? Is it a good idea? I will run through various analysis transforming raw data into clean data sets that I explore and use machine learning techniques to gleam new insights. </p>

    </div>
  </div>
</body>
</html>
